{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LbQC9Wc6dmg"
      },
      "outputs": [],
      "source": [
        "from typing import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set_style(\"whitegrid\")\n",
        "import requests\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions import Distribution\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from functools import reduce\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "\n",
        "import gzip\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "try:\n",
        "    from plotting import make_vae_plots\n",
        "except:\n",
        "    try:\n",
        "        # Download the file\n",
        "        url = \"https://raw.githubusercontent.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/master/7_Unsupervised/plotting.py\"\n",
        "        response = requests.get(url)\n",
        "        with open(\"plotting.py\", \"w\") as file:\n",
        "            file.write(response.text)\n",
        "\n",
        "        # Try importing again\n",
        "        from plotting import make_vae_plots\n",
        "    except Exception as ex:\n",
        "        print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
        "              \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
        "              \\n---------------------------------------------\")\n",
        "        print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_chunk(filename, chunk_size=1000):\n",
        "    \"\"\" Load a chunk of data from a gzipped TSV file. \"\"\"\n",
        "    return pd.read_csv(filename, sep='\\t', compression='gzip', chunksize=chunk_size)\n",
        "\n",
        "def separate_ids_and_data(data):\n",
        "    ids = data.iloc[:, 0]\n",
        "    data = data.iloc[:, 1:]\n",
        "    return ids, data\n",
        "\n",
        "class ReparameterizedDiagonalGaussian(Distribution):\n",
        "    \"\"\"\n",
        "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
        "    \"\"\"\n",
        "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
        "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
        "        self.mu = mu\n",
        "        self.sigma = log_sigma.exp()\n",
        "\n",
        "    def sample_epsilon(self) -> Tensor:\n",
        "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
        "        return torch.empty_like(self.mu).normal_()\n",
        "\n",
        "    def sample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.rsample()\n",
        "\n",
        "    def rsample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
        "        # z = mu + sigma * epsilon\n",
        "        return self.mu + self.sigma * self.sample_epsilon()\n",
        "\n",
        "    def log_prob(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
        "        # Log probability for Gaussian distribution\n",
        "        # log p(z) = -1/2 * [log(2*pi) + 2*log(sigma) + (z - mu)^2/sigma^2]\n",
        "        return -0.5 * (torch.log(2 * torch.tensor(math.pi)) + 2 * torch.log(self.sigma) +\n",
        "                       torch.pow(z - self.mu, 2) / torch.pow(self.sigma, 2))\n",
        "    \n",
        "    def count_csv_rows(filename):\n",
        "        # If the file is gzip-compressed, decompress it first\n",
        "        if filename.endswith('.gz'):\n",
        "            with gzip.open(filename, 'rt', newline='') as csvfile:\n",
        "                row_count = sum(1 for row in csvfile)\n",
        "        else:\n",
        "            # Specify the correct encoding (e.g., 'utf-8', 'latin-1', etc.)\n",
        "            encoding = 'utf-8'  # Change to the appropriate encoding if needed\n",
        "            with open(filename, 'r', newline='', encoding=encoding) as csvfile:\n",
        "                row_count = sum(1 for row in csvfile)\n",
        "        return row_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbMPs6aIq267",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKE6MiTPq267",
        "outputId": "a22fa10f-d1fd-473b-bb30-70ddff0601fe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18965, 156958)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the file paths\n",
        "archs4_path = \"/dtu-compute/datasets/iso_02456/archs4_gene_expression_norm_transposed.tsv.gz\"\n",
        "gtex_gene_path = \"/dtu-compute/datasets/iso_02456/gtex_gene_expression_norm_transposed.tsv.gz\"\n",
        "gtex_isoform_path = \"/dtu-compute/datasets/iso_02456/gtex_isoform_expression_norm_transposed.tsv.gz\"\n",
        "gtex_anno_path = \"/dtu-compute/datasets/iso_02456/gtex_gene_isoform_annoation.tsv.gz\"\n",
        "gtex_tissue_path = \"/dtu-compute/datasets/iso_02456/gtex_annot.tsv.gz\"\n",
        "\n",
        "\n",
        "gtex_gene_chunk = next(load_data_chunk(gtex_gene_path, chunk_size=10))\n",
        "gtex_isoform_chunk = next(load_data_chunk(gtex_isoform_path, chunk_size=10))\n",
        "\n",
        "gene_ids, gene_data = separate_ids_and_data(gtex_gene_chunk)\n",
        "isoform_ids, isoform_data = separate_ids_and_data(gtex_isoform_chunk)\n",
        "\n",
        "num_genes = gene_data.shape[1]\n",
        "num_isoforms = isoform_data.shape[1]\n",
        "\n",
        "num_genes, num_isoforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gtex_gene_path num rows: 17357\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8.6785"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# count_csv_rows(gtex_gene_path)\n",
        "print(\"gtex_gene_path num rows: 17357\")\n",
        "percentage_calc = 17357 * 0.0005\n",
        "percentage_calc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using percentage 0.001 should load 17 samples, and 0.01 170. We use 0.005 for 86 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Label Encoding: This converts each unique label into a unique integer. If you have a large number of classes, be aware of memory usage.\n",
        "- Inverse Transformation: If you need to get back the original labels from the encoded ones, you can use self.label_encoder.inverse_transform().\n",
        "- Data Types: Labels are converted to torch.long since they are now integers, which is a common practice for categorical labels in classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeneExpressionDataset(Dataset):\n",
        "    def __init__(self, filepath, percentage=0.1):\n",
        "        self.data = self.load_data_percentage(filepath, percentage)\n",
        "        # Assuming the first column is the label\n",
        "        self.labels = self.data.iloc[:, 0]\n",
        "        self.genes = self.data.iloc[:, 1:]\n",
        "\n",
        "        # Encode the labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        label = self.encoded_labels[idx]\n",
        "        genes = self.genes.iloc[idx]\n",
        "        return torch.tensor(genes.values, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def get_original_labels(self, encoded_labels):\n",
        "        return self.label_encoder.inverse_transform(encoded_labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data_percentage(filepath, percentage=0.1):\n",
        "        # Load a percentage of the data as shown previously\n",
        "        cols = pd.read_csv(filepath, sep='\\t', compression='gzip', nrows=0).columns\n",
        "        n_total_rows = sum(1 for row in open(filepath, 'rb'))\n",
        "        n_rows_to_load = int(n_total_rows * percentage)\n",
        "        skip_rows = np.random.choice(np.arange(1, n_total_rows), size=n_total_rows - n_rows_to_load, replace=False)\n",
        "        return pd.read_csv(filepath, sep='\\t', compression='gzip', usecols=cols, skiprows=skip_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "gene_dataset = GeneExpressionDataset(gtex_gene_path, percentage=0.0005)\n",
        "gene_train_loader = DataLoader(gene_dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[3.2063, 1.1440, 0.0000,  ..., 0.0000, 0.2750, 0.0286],\n",
              "         [2.2540, 1.7866, 0.0000,  ..., 0.2141, 0.5753, 0.0000],\n",
              "         [2.7312, 0.0286, 0.2388,  ..., 0.4222, 0.2016, 0.0000],\n",
              "         ...,\n",
              "         [3.2795, 0.2510, 0.3334,  ..., 0.6415, 0.1110, 0.0000],\n",
              "         [1.7570, 2.7592, 0.0286,  ..., 0.0000, 0.2987, 0.0286],\n",
              "         [2.1699, 0.9855, 0.5160,  ..., 0.3896, 0.7740, 0.0841]]),\n",
              " tensor([ 9,  2, 10,  5,  7,  3,  6,  0,  8,  4,  1]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "genes, labels = next(iter(gene_train_loader))\n",
        "genes, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get original labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['GTEX-ZPIC-0426-SM-DO91V', 'GTEX-145MF-2226-SM-7EPIR',\n",
              "       'GTEX-ZVZQ-0008-SM-51MSK', 'GTEX-1S82Y-0005-SM-DO123',\n",
              "       'GTEX-S4UY-0726-SM-4AD6X', 'GTEX-1HSEH-0526-SM-ADEJ1',\n",
              "       'GTEX-QCQG-0526-SM-48U2A', 'GTEX-111FC-0326-SM-5GZZ1',\n",
              "       'GTEX-T5JW-0008-SM-4DM5X', 'GTEX-1JKYR-1226-SM-CM2S7',\n",
              "       'GTEX-11VI4-2126-SM-5EGI1'], dtype=object)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert encoded labels back to original string labels\n",
        "original_labels = gene_dataset.get_original_labels(labels.numpy())\n",
        "original_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DuRG3tVq268",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Building the model\n",
        "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRRu-aq7q268",
        "outputId": "f769477a-51a5-4325-934a-89edcdb4ff54",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VariationalAutoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=18965, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=200, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=18965, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    \"\"\"A Variational Autoencoder with\n",
        "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
        "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
        "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.latent_features = latent_features\n",
        "        self.observation_features = np.prod(input_shape)\n",
        "\n",
        "\n",
        "        # Inference Network\n",
        "        # Encode the observation `x` into the parameters of the posterior distribution\n",
        "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
        "            nn.Linear(in_features=128, out_features=2*latent_features) # <- note the 2*latent_features\n",
        "        )\n",
        "\n",
        "        # Generative Model\n",
        "        # Decode the latent sample `z` into the parameters of the observation model\n",
        "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_features, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=self.observation_features)\n",
        "        )\n",
        "\n",
        "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
        "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
        "\n",
        "    def posterior(self, x:Tensor) -> Distribution:\n",
        "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
        "\n",
        "        # compute the parameters of the posterior\n",
        "        h_x = self.encoder(x)\n",
        "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
        "\n",
        "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "\n",
        "    def prior(self, batch_size:int=1)-> Distribution:\n",
        "        \"\"\"return the distribution `p(z)`\"\"\"\n",
        "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
        "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
        "\n",
        "        # return the distribution `p(z)`\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "\n",
        "    def observation_model(self, z:Tensor) -> Distribution:\n",
        "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
        "        px_logits = self.decoder(z)\n",
        "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
        "        return Bernoulli(logits=px_logits, validate_args=False)\n",
        "\n",
        "\n",
        "    def forward(self, x) -> Dict[str, Any]:\n",
        "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
        "\n",
        "        # flatten the input\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # define the posterior q(z|x) / encode x into q(z|x)\n",
        "        qz = self.posterior(x)\n",
        "\n",
        "        # define the prior p(z)\n",
        "        pz = self.prior(batch_size=x.size(0))\n",
        "\n",
        "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
        "        z = qz.rsample()\n",
        "\n",
        "        # define the observation model p(x|z) = B(x | g(z))\n",
        "        px = self.observation_model(z)\n",
        "\n",
        "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
        "\n",
        "\n",
        "    def sample_from_prior(self, batch_size:int=100):\n",
        "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
        "\n",
        "        # degine the prior p(z)\n",
        "        pz = self.prior(batch_size=batch_size)\n",
        "\n",
        "        # sample the prior\n",
        "        z = pz.rsample()\n",
        "\n",
        "        # define the observation model p(x|z) = B(x | g(z))\n",
        "        px = self.observation_model(z)\n",
        "\n",
        "        return {'px': px, 'pz': pz, 'z': z}\n",
        "\n",
        "\n",
        "latent_features = 100\n",
        "vae = VariationalAutoencoder(genes[0].shape, latent_features)\n",
        "print(vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBiGYcCOq268",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Implement a module for Variational Inference\n",
        "\n",
        "**Exercise 1**: implement `elbo` ($\\mathcal{L}$) and `beta_elbo` ($\\mathcal{L}^\\beta$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rDM3tYQsq268",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def reduce(x:Tensor) -> Tensor:\n",
        "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
        "    return x.view(x.size(0), -1).sum(dim=1)\n",
        "\n",
        "class VariationalInference(nn.Module):\n",
        "    def __init__(self, beta:float=1.):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
        "\n",
        "        # forward pass through the model\n",
        "        outputs = model(x)\n",
        "\n",
        "        # unpack outputs\n",
        "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
        "\n",
        "        # evaluate log probabilities\n",
        "        log_px = reduce(px.log_prob(x))\n",
        "        log_pz = reduce(pz.log_prob(z))\n",
        "        log_qz = reduce(qz.log_prob(z))\n",
        "\n",
        "        # compute the ELBO with and without the beta parameter:\n",
        "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
        "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
        "        kl = log_qz - log_pz\n",
        "        elbo = log_px - kl # <- your code here\n",
        "        beta_elbo = log_px - self.beta * kl # <- your code here\n",
        "\n",
        "        # loss\n",
        "        loss = -beta_elbo.mean()\n",
        "\n",
        "        # prepare the output\n",
        "        with torch.no_grad():\n",
        "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
        "\n",
        "        return loss, diagnostics, outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amy31U7Dq269",
        "outputId": "ce16e4b6-c236-4066-a43a-105e82b502b1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss   | mean =  13196.504, shape: []\n",
            "elbo   | mean = -13196.504, shape: [11]\n",
            "log_px | mean = -13176.575, shape: [11]\n",
            "kl     | mean =     19.927, shape: [11]\n"
          ]
        }
      ],
      "source": [
        "vi = VariationalInference(beta=1.0)\n",
        "loss, diagnostics, outputs = vi(vae, genes)\n",
        "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
        "for key, tensor in diagnostics.items():\n",
        "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5NZoEyq269",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "### Initialize the model, evaluator and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xzfM_yWWq269",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "# define the models, evaluator and optimizer\n",
        "\n",
        "# VAE\n",
        "latent_features = 2\n",
        "vae = VariationalAutoencoder(genes[0].shape, latent_features)\n",
        "\n",
        "# Evaluator: Variational Inference\n",
        "beta = 1\n",
        "vi = VariationalInference(beta=beta)\n",
        "\n",
        "# The Adam optimizer works really well with VAEs.\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "# define dictionary to store the training curves\n",
        "training_data = defaultdict(list)\n",
        "validation_data = defaultdict(list)\n",
        "\n",
        "epoch = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl-NPl9xSo32"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "#### Plotting Guide\n",
        "\n",
        "- **1st Row**: Reproducing the figure from the beginning of the Notebook.\n",
        "  - (Left) Data.\n",
        "  - (Middle) Latent space: the large gray disk represents the prior (radius = $2\\sigma$), each point represents a latent sample $\\mathbf{z}$. The smaller ellipses represent the distributions $q_\\phi(\\mathbf{z} | \\mathbf{x})$ (radius = $2\\sigma$). When using $\\geq 2$ latent features, dimensionality reduction is applied using t-SNE, and only samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ are displayed.\n",
        "  - (Right) Samples from $p_\\theta(\\mathbf{x} | \\mathbf{z}$).\n",
        "\n",
        "- **2nd Row**: Training curves.\n",
        "\n",
        "- **3rd Row**: Latent samples.\n",
        "  - (Left) Prior samples $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim p(\\mathbf{z})$.\n",
        "  - (Middle) Latent Interpolations. For each row: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | t \\cdot \\mathbf{z}_1 + (1-t) \\cdot \\mathbf{z}_2), \\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z}), t=0 \\dots 1$.\n",
        "  - (Right): Sampling $\\mathbf{z}$ from a grid [-3:3, -3:3] $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim \\text{grid}(-3:3, -3:3)$ (only available for 2D latent space).\n",
        "\n",
        "**NOTE**: This may take a while on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.2+cu102\n",
            "10.2\n",
            ">> Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\">> Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = gene_train_loader\n",
        "test_loader = gene_test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E1eeqrcvtnWf",
        "outputId": "91d1a533-5896-4cb8-8c12-ad0d958175f9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Using device: cpu\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-96e8fddd4f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Go through each batch in the training dataset using the loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Note that y is not necessarily known as it is here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "num_epochs = 5 # 100\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\">> Using device: {device}\")\n",
        "\n",
        "# move the model to the device\n",
        "vae = vae.to(device)\n",
        "\n",
        "# training..\n",
        "while epoch < num_epochs:\n",
        "    epoch+= 1\n",
        "    training_epoch_data = defaultdict(list)\n",
        "    vae.train()\n",
        "\n",
        "    # Go through each batch in the training dataset using the loader\n",
        "    # Note that y is not necessarily known as it is here\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "\n",
        "        # perform a forward pass through the model and compute the ELBO\n",
        "        loss, diagnostics, outputs = vi(vae, x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # gather data for the current bach\n",
        "        for k, v in diagnostics.items():\n",
        "            training_epoch_data[k] += [v.mean().item()]\n",
        "\n",
        "\n",
        "    # gather data for the full epoch\n",
        "    for k, v in training_epoch_data.items():\n",
        "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
        "\n",
        "    # Evaluate on a single batch, do not propagate gradients\n",
        "    with torch.no_grad():\n",
        "        vae.eval()\n",
        "\n",
        "        # Just load a single batch from the test loader\n",
        "        x, y = next(iter(test_loader))\n",
        "        x = x.to(device)\n",
        "\n",
        "        # perform a forward pass through the model and compute the ELBO\n",
        "        loss, diagnostics, outputs = vi(vae, x)\n",
        "\n",
        "        # gather data for the validation step\n",
        "        for k, v in diagnostics.items():\n",
        "            validation_data[k] += [v.mean().item()]\n",
        "\n",
        "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
        "    make_vae_plots(vae, x, y, outputs, training_data, validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmxAP4ZasMB"
      },
      "source": [
        "Summary:\n",
        "- Subplots using posterior samples:\n",
        " - Latent Samples (Row 1, Column 2)\n",
        " - Reconstruction (Row 1, Column 3)\n",
        "- Subplots using prior samples:\n",
        " - Samples (Row 3, Column 1)\n",
        " - Latent Interpolations (Row 3, Column 2)\n",
        " - Samples from a grid (Row 3, Column 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQVNsTCBCgQq"
      },
      "source": [
        "Define training function to avoid repeating code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK5oVJgTCfur"
      },
      "outputs": [],
      "source": [
        "def train_vae_model(vae, train_loader, test_loader, optimizer, num_epochs=100, device=\"cpu\", beta=1.0):\n",
        "    \"\"\"\n",
        "    Train the VAE model and evaluate.\n",
        "\n",
        "    Parameters:\n",
        "    - vae: The VAE model to train.\n",
        "    - train_loader: DataLoader for the training data.\n",
        "    - test_loader: DataLoader for the test data.\n",
        "    - optimizer: Optimizer to use for training.\n",
        "    - num_epochs: Number of epochs to train for.\n",
        "    - device: Device to train on. Either \"cpu\" or \"cuda\".\n",
        "\n",
        "    Returns:\n",
        "    - training_data: Dictionary containing training diagnostics.\n",
        "    - validation_data: Dictionary containing validation diagnostics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the evaluator and data storage for diagnostics\n",
        "    vi = VariationalInference(beta=beta)\n",
        "    training_data = defaultdict(list)\n",
        "    validation_data = defaultdict(list)\n",
        "    epoch = 0\n",
        "\n",
        "    # Move the model to the desired device\n",
        "    vae = vae.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    while epoch < num_epochs:\n",
        "        epoch += 1\n",
        "        training_epoch_data = defaultdict(list)\n",
        "        vae.train()\n",
        "\n",
        "        # Training step\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device)\n",
        "            loss, diagnostics, outputs = vi(vae, x)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            for k, v in diagnostics.items():\n",
        "                training_epoch_data[k] += [v.mean().item()]\n",
        "\n",
        "        for k, v in training_epoch_data.items():\n",
        "            training_data[k] += [np.mean(training_epoch_data[k])]\n",
        "\n",
        "        # Validation step\n",
        "        with torch.no_grad():\n",
        "            vae.eval()\n",
        "            x, y = next(iter(test_loader))\n",
        "            x = x.to(device)\n",
        "            loss, diagnostics, outputs = vi(vae, x)\n",
        "            for k, v in diagnostics.items():\n",
        "                validation_data[k] += [v.mean().item()]\n",
        "\n",
        "        # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
        "        make_vae_plots(vae, x, y, outputs, training_data, validation_data)\n",
        "\n",
        "    return training_data, validation_data\n",
        "\n",
        "latent_features = 2\n",
        "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# training_data, validation_data = train_vae_model(vae, train_loader, test_loader, optimizer, num_epochs=100, device=device, beta=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PWnLMrHq269"
      },
      "source": [
        "# Analyzing the VAE\n",
        "\n",
        "## Mandatory Exercises\n",
        "\n",
        "### Exercise 1.\n",
        "\n",
        "1. Implement the class `ReparameterizedDiagonalGaussian` (`log_prob()` and `rsample()`).\n",
        "2. Import the class `Bernoulli`\n",
        "3. Implement the class `VariationalInference` (computation of the `elbo` and `beta_elbo`).\n",
        "\n",
        "### Exercise 2.\n",
        "\n",
        "**Trainnig and Evaluating a VAE model**\n",
        "\n",
        "1. Why do we use the reparameterization trick?\n",
        "2. What available metric can you use to estimate the marginal likelihood ($p_\\theta(\\mathbf{x})$) ?\n",
        "3. In the above plots, we display numerous model samples. If you had to pick one plot, which one would you pick to evaluate the quality of a VAE (i.e. using posterior samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ or prior samples $\\mathbf{z} \\sim p(\\mathbf{z})$) ? Why?.\n",
        "4. How could you exploit the VAE model for classification?\n",
        "\n",
        "**Answers**:\n",
        "1. The reparameterization trick is used to make the variational autoencoder's latent variables differentiable, allowing for backpropagation during the training process.\n",
        "2. Monte-Carlo estimate is computationally challenging due to its high variance and the numerically instability, so we can use ELBO.\n",
        "3.  The choice between the two depends on the primary goal of using the VAE:\n",
        " - If the primary goal is data reconstruction, then evaluating using the posterior samples would be more relevant. The quality and fidelity of reconstructed images would be the focus.\n",
        " - If the primary goal is data generation (e.g., generating new images that resemble the training data distribution), then evaluating using the prior samples would be more appropriate. The diversity, novelty, and quality of generated images would be the focus.\n",
        "\n",
        " However, for a holistic evaluation of a VAE, which aims to both encode and decode data efficiently, I would lean towards the posterior samples. This is because the core functionality of a VAE is its ability to encode data into a latent space and then decode it back. If this reconstruction is poor, then even if it generates reasonable outputs from the prior, it's not effectively serving its dual purpose. So I would pick the reconstruction plot (Row 1, Column 3). However, I personally really like the plot (Row 3, Column 3) to get an idea of the latent space.\n",
        "4. A couple ways to leverage a VAE for classification tasks are:\n",
        " - Latent Space Classification for Dimensionality Reduction: Once the VAE is trained, we can encode the data into the latent space and then train a classifier (like an SVM or neural network) on the latent representations. This dimensionality reduction could help reduce overfitting and model complexity.\n",
        " - Semi-supervised Learning: VAEs can be particularly beneficial in semi-supervised scenarios, where labeled data is limited. The generative aspect of the VAE can leverage unlabeled data to improve the quality of the latent space, which can then aid classification on the small labeled dataset.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQxM0tsdt7W"
      },
      "source": [
        "### Exercise 3.\n",
        "\n",
        "**Experiment with the VAE model.**\n",
        "\n",
        "1. Experiment with the number of layers and activation functions in order to improve the reconstructions and latent representation. What solution did you find the best and why?\n",
        "2. Try to increase the number of digit classes in the training set and analyze the learning curves, latent space and reconstructions. For which classes and why does the VAE fail in reconstructing?  *HINT: Try the combination: `classes=[0, 1, 4, 9]`, to see how well VAE can separate these digits in the latent representation and reconstructions.*\n",
        "3. Increase the number of units in the latent layer. Does it increase the models representational power and how can you see and explain this? How does this affect the quality of the reconstructions?\n",
        "\n",
        "**Answers**:\n",
        "\n",
        "1. We tried going deeper by adding 1 and 2 layers without seeing much difference. However, if we go shallower by removing one layer both the reconstruction and generative results get worse, though we can still recongnize some digits.\n",
        "As for the activation functions we tried LeakyRelu with pretty much the same results, maybe slightly worst. The plots are available at the end of the notebook.\n",
        "2. The VAE struggles reconstructing the digit 9 and while some generated digits look correct, others don't. Again, the results for this and further tests are available in the Annex at the end of the notebook.\n",
        "3. Increasing the number of units in the latent layer to 4 improved the model reconstruction and generation capabilities, where it still has some trouble differentiating between 9 and 4 but it performs better than before. This happens because when we increase the units, the latent space becomes capable of capturing more complex patterns and nuances in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK2sDSOUdqKS"
      },
      "source": [
        "### Exercise 4.\n",
        "\n",
        "**Analyze the purpose of the KL-term and the $\\beta$ parameter.**\n",
        "\n",
        "1. How does the KL-term, $\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$, work as a regulariser on the distributions over latent variables? *HINT*: When maximising the ELBO, the probability-distance measure is minimised $\\operatorname{KL} \\rightarrow 0$ so that $q(z|x) \\rightarrow p(z) = \\mathcal{N}(z|0,I)$ for all examples, x. At $\\operatorname{KL} = 0$ variations in x stops having an affect on the latent distribution and latent units are all described by the same distribution, $\\mathcal{N}(z|0,I)$, so they produce a noisy output without signal (i.e. $\\mathbf{z}=\\epsilon \\sim \\mathcal{N}(0,I)$) to the decoder.\n",
        "2. Try removing the KL-term (using the $\\beta$ parameter) and analyze what happens during training, in the learning curves, latent representation and reconstructions compared to before removing it.\n",
        "3. What does the loss reduces to? Explain how this can affect a VAE. *HINT*: Compare loss function for AE and VAE, and remember that we can use the pixel-wise binary crossentropy error as the loss in the AEs and for the reconstruction error, $\\log p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\log \\mathcal{B}(\\mathbf{x} | g_\\theta(z))$, in VAEs.\n",
        "4. Experiment with different $\\beta$ values (e.g. [0.1, 10, 50]) and explain how this affects the reconstruction quality and the latent representations. *HINT* what is the tradeoff between reconstruction error ($\\log p_\\theta(\\mathbf{x} | \\mathbf{z})$) and the KL term?\n",
        "\n",
        "**Answers**:\n",
        "\n",
        "1. The KL-term, measures the divergence between two distributions: the approximate posterior and the prior, which is typically a standard normal distribution N(0,I). As the ELBO is maximized, the KL-term aims to be minimized. When it reaches 0, it indicates that the approximate posterior is the same as the prior. This means that the latent variables are not being influenced by the input data x and are essentially just noise sampled from N(0,I). Thus, the KL-term acts as a regularizer by ensuring that the latent representations don't stray too far from a prior belief (which is the standard normal distribution in most VAEs). Without this term, the latent variables might fit too closely to the training data, leading to overfitting.\n",
        "2. By removing the KL-term (setting $\\beta$=0), the VAE doesn't have any regularization on the latent space. This means the model focuses solely on the reconstruction error. We can observe:\n",
        " - The latent representation is probably more overfitted to the training data.\n",
        " - The reconstructions might be sharper initially but could lead to overfitting, where it performs well on training data but poorly on unseen data.\n",
        "\n",
        " Also for this part we experimented errors during the execution, only for beta=0. When we went above 40 epochs, most of the times we got a Cuda error that can be seen in the Annex. Not sure of why this happens and if the student reading this have any idea I would appreciate the feedback.\n",
        "3. Without the KL-term, the VAE loss reduces to just the reconstruction error. In this scenario, the VAE becomes more like a traditional autoencoder (AE). This can lead to overfitting more easily just like an AE.\n",
        "4. We can see the following differences:\n",
        " - A smaller $\\beta$ (e.g., 0.1): Puts less emphasis on the KL-term, leading to better reconstructions but poorer generalization and more complex latent representations.\n",
        " - $\\beta$ =1: This is the standard VAE, which balances reconstruction with regularization from the KL-term.\n",
        " - A larger $\\beta$ (e.g., 10, 50): Puts more emphasis on the KL-term, leading to simpler latent representations but potentially worse reconstructions. But for $\\beta$=50 specifically, both reconstructions and generations are diffuse and bad.\n",
        "\n",
        " The trade-off is that As $\\beta$ increases, the model prioritizes making the approximate posterior close to the prior, often at the expense of reconstruction quality. Conversely, as $\\beta$ decreases, the model prioritizes reconstruction, potentially at the expense of generalization.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
