{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LbQC9Wc6dmg"
      },
      "outputs": [],
      "source": [
        "from typing import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "# sns.set_style(\"whitegrid\")\n",
        "import requests\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions import Distribution\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from functools import reduce\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "from torch import optim\n",
        "\n",
        "import gzip\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import h5py\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_chunk(filename, chunk_size=1000):\n",
        "    \"\"\" Load a chunk of data from a gzipped TSV file. \"\"\"\n",
        "    return pd.read_csv(filename, sep='\\t', compression='gzip', chunksize=chunk_size)\n",
        "\n",
        "def separate_ids_and_data(data):\n",
        "    ids = data.iloc[:, 0]\n",
        "    data = data.iloc[:, 1:]\n",
        "    return ids, data\n",
        "\n",
        "class ReparameterizedDiagonalGaussian(Distribution):\n",
        "    \"\"\"\n",
        "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
        "    \"\"\"\n",
        "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
        "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
        "        self.mu = mu\n",
        "        self.sigma = log_sigma.exp()\n",
        "\n",
        "    def sample_epsilon(self) -> Tensor:\n",
        "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
        "        return torch.empty_like(self.mu).normal_()\n",
        "\n",
        "    def sample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.rsample()\n",
        "\n",
        "    def rsample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
        "        # z = mu + sigma * epsilon\n",
        "        return self.mu + self.sigma * self.sample_epsilon()\n",
        "\n",
        "    def log_prob(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
        "        # Log probability for Gaussian distribution\n",
        "        # log p(z) = -1/2 * [log(2*pi) + 2*log(sigma) + (z - mu)^2/sigma^2]\n",
        "        return -0.5 * (torch.log(2 * torch.tensor(math.pi)) + 2 * torch.log(self.sigma) +\n",
        "                       torch.pow(z - self.mu, 2) / torch.pow(self.sigma, 2))\n",
        "    \n",
        "    def count_csv_rows(filename):\n",
        "        # If the file is gzip-compressed, decompress it first\n",
        "        if filename.endswith('.gz'):\n",
        "            with gzip.open(filename, 'rt', newline='') as csvfile:\n",
        "                row_count = sum(1 for row in csvfile)\n",
        "        else:\n",
        "            # Specify the correct encoding (e.g., 'utf-8', 'latin-1', etc.)\n",
        "            encoding = 'utf-8'  # Change to the appropriate encoding if needed\n",
        "            with open(filename, 'r', newline='', encoding=encoding) as csvfile:\n",
        "                row_count = sum(1 for row in csvfile)\n",
        "        return row_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbMPs6aIq267",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKE6MiTPq267",
        "outputId": "a22fa10f-d1fd-473b-bb30-70ddff0601fe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18965, 156958)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the file paths\n",
        "archs4_path = \"/dtu-compute/datasets/iso_02456/archs4_gene_expression_norm_transposed.tsv.gz\"\n",
        "gtex_gene_path = \"/dtu-compute/datasets/iso_02456/gtex_gene_expression_norm_transposed.tsv.gz\"\n",
        "gtex_isoform_path = \"/dtu-compute/datasets/iso_02456/gtex_isoform_expression_norm_transposed.tsv.gz\"\n",
        "gtex_anno_path = \"/dtu-compute/datasets/iso_02456/gtex_gene_isoform_annoation.tsv.gz\"\n",
        "gtex_tissue_path = \"/dtu-compute/datasets/iso_02456/gtex_annot.tsv.gz\"\n",
        "\n",
        "num_genes = 18965\n",
        "num_isoforms = 156958\n",
        "\n",
        "num_genes, num_isoforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gtex_gene_path num rows: 17357\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8.6785"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# count_csv_rows(gtex_gene_path)\n",
        "print(\"gtex_gene_path num rows: 17357\")\n",
        "percentage_calc = 17357 * 0.0005\n",
        "percentage_calc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using percentage 0.001 should load 17 samples, and 0.01 170. We use 0.005 for 86 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Label Encoding: This converts each unique label into a unique integer. If you have a large number of classes, be aware of memory usage.\n",
        "- Inverse Transformation: If you need to get back the original labels from the encoded ones, you can use self.label_encoder.inverse_transform().\n",
        "- Data Types: Labels are converted to torch.long since they are now integers, which is a common practice for categorical labels in classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "GTEX_NUM_ROWS = 17356\n",
        "\n",
        "class GeneExpressionDataset(Dataset):\n",
        "    \"\"\" # Old dataLoader with no TestSet\n",
        "    def __init__(self, filepath, percentage=0.1):\n",
        "        self.data = self.load_data_percentage(filepath, percentage)\n",
        "        # Assuming the first column is the label\n",
        "        self.labels = self.data.iloc[:, 0]\n",
        "        self.genes = self.data.iloc[:, 1:]\n",
        "\n",
        "        # Encode the labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, labels):\n",
        "        self.labels = labels\n",
        "        self.genes = data\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.genes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        label = self.encoded_labels[idx]\n",
        "        genes = self.genes.iloc[idx]\n",
        "        return torch.tensor(genes.values, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def get_original_labels(self, encoded_labels):\n",
        "        return self.label_encoder.inverse_transform(encoded_labels)\n",
        "\n",
        "    # Next function is slow because it counts all the rows to calculate the percentage, that is why we added load rows\n",
        "    @staticmethod\n",
        "    def load_data_percentage(filepath, percentage=0.1):\n",
        "        # Load a percentage of the data as shown previously\n",
        "        cols = pd.read_csv(filepath, sep='\\t', compression='gzip', nrows=0).columns\n",
        "        n_total_rows = sum(1 for row in open(filepath, 'rb'))\n",
        "        n_rows_to_load = int(n_total_rows * percentage)\n",
        "        skip_rows = np.random.choice(np.arange(1, n_total_rows), size=n_total_rows - n_rows_to_load, replace=False)\n",
        "        return pd.read_csv(filepath, sep='\\t', compression='gzip', usecols=cols, skiprows=skip_rows)\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_rows(filepath, num_rows):\n",
        "        \"\"\"\n",
        "        Load a specific number of rows from the file.\n",
        "        \"\"\"\n",
        "        return pd.read_csv(filepath, sep='\\t', compression='gzip', nrows=num_rows)\n",
        "\n",
        "class GtexDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir:str=\"/dtu-compute/datasets/iso_02456/hdf5/\", include:str=\"\", exclude:str=\"\", load_in_mem:bool=False):\n",
        "        f_gtex_gene = h5py.File(data_dir + 'gtex_gene_expression_norm_transposed.hdf5', mode='r')\n",
        "        f_gtex_isoform = h5py.File(data_dir + 'gtex_isoform_expression_norm_transposed.hdf5', mode='r')\n",
        "\n",
        "        self.dset_gene = f_gtex_gene['expressions']\n",
        "        self.dset_isoform = f_gtex_isoform['expressions']\n",
        "\n",
        "        assert(self.dset_gene.shape[0] == self.dset_isoform.shape[0])\n",
        "\n",
        "        if load_in_mem:\n",
        "            self.dset_gene = np.array(self.dset_gene)\n",
        "            self.dset_isoform = np.array(self.dset_isoform)\n",
        "\n",
        "        self.idxs = None\n",
        "\n",
        "        if include and exclude:\n",
        "            raise ValueError(\"You can only give either the 'include_only' or the 'exclude_only' argument.\")\n",
        "\n",
        "        if include:\n",
        "            matches = [bool(re.search(include, s.decode(), re.IGNORECASE)) for s in f_gtex_gene['tissue']]\n",
        "            self.idxs = np.where(matches)[0]\n",
        "\n",
        "        elif exclude:\n",
        "            matches = [not(bool(re.search(exclude, s.decode(), re.IGNORECASE))) for s in f_gtex_gene['tissue']]\n",
        "            self.idxs = np.where(matches)[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.idxs is None:\n",
        "            return self.dset_gene.shape[0]\n",
        "        else:\n",
        "            return self.idxs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.idxs is None:\n",
        "            return self.dset_gene[idx], self.dset_isoform[idx]\n",
        "        else:\n",
        "            return self.dset_gene[self.idxs[idx]], self.dset_isoform[self.idxs[idx]]\n",
        "        \n",
        "class PartialDataset(Dataset):\n",
        "    def __init__(self, data: Dataset, start: int, end: int):\n",
        "        assert start >= 0\n",
        "        assert end < len(data)\n",
        "\n",
        "        self.data = data\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.end - self.start + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[self.start + idx]\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data loaded.\n",
            "Splitting data into training and testing sets...\n",
            "Data split.\n",
            "Datasets created.\n",
            "Data loaders created.\n"
          ]
        }
      ],
      "source": [
        "num_rows_to_load = 100  # specify the number of rows you want to load\n",
        "\n",
        "# Print progress\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# Load the entire dataset\n",
        "full_data = GtexDataset()\n",
        "# Print progress\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "print(\"Splitting data into training and testing sets...\")\n",
        "gene_train_dataset = PartialDataset(full_data, 0, 500)\n",
        "gene_test_dataset = PartialDataset(full_data, 501, 520)\n",
        "\n",
        "# Print progress\n",
        "print(\"Data split.\")\n",
        "\n",
        "# # Create dataset instances for training and testing\n",
        "# print(\"Creating training and testing datasets...\")\n",
        "# gene_train_dataset = GeneExpressionDataset(train_data.iloc[:, 1:], train_data.iloc[:, 0])\n",
        "# gene_test_dataset = GeneExpressionDataset(test_data.iloc[:, 1:], test_data.iloc[:, 0])\n",
        "\n",
        "# Print progress\n",
        "print(\"Datasets created.\")\n",
        "\n",
        "gene_train_loader = DataLoader(gene_train_dataset, batch_size=64, shuffle=True)\n",
        "gene_test_loader = DataLoader(gene_test_dataset, batch_size=64, shuffle=False)  # Usually, shuffling is not needed for testing\n",
        "\n",
        "# Print progress\n",
        "print(\"Data loaders created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nload_percentage = 0.0005\\n\\n# Load the entire dataset\\nfull_data = GeneExpressionDataset.load_data_percentage(gtex_gene_path, percentage=load_percentage)\\n\\n# Split the data into training and testing sets\\ntrain_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\\n\\n# Create dataset instances for training and testing\\ngene_train_dataset = GeneExpressionDataset(train_data.iloc[:, 1:], train_data.iloc[:, 0])\\ngene_test_dataset = GeneExpressionDataset(test_data.iloc[:, 1:], test_data.iloc[:, 0])\\n\\ngene_train_loader = DataLoader(gene_train_dataset, batch_size=64, shuffle=True)\\ngene_test_loader = DataLoader(gene_test_dataset, batch_size=64, shuffle=False)  # Usually, shuffling is not needed for testing\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Same code with percentage and without showing progress \n",
        "\"\"\"\n",
        "load_percentage = 0.0005\n",
        "\n",
        "# Load the entire dataset\n",
        "full_data = GeneExpressionDataset.load_data_percentage(gtex_gene_path, percentage=load_percentage)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataset instances for training and testing\n",
        "gene_train_dataset = GeneExpressionDataset(train_data.iloc[:, 1:], train_data.iloc[:, 0])\n",
        "gene_test_dataset = GeneExpressionDataset(test_data.iloc[:, 1:], test_data.iloc[:, 0])\n",
        "\n",
        "gene_train_loader = DataLoader(gene_train_dataset, batch_size=64, shuffle=True)\n",
        "gene_test_loader = DataLoader(gene_test_dataset, batch_size=64, shuffle=False)  # Usually, shuffling is not needed for testing\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.GtexDataset at 0x7f0ec7edf650>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[2.1827, 3.0108, 0.9561,  ..., 0.2750, 1.5801, 0.6041],\n",
              "         [1.5410, 3.1210, 0.1506,  ..., 0.0000, 2.4249, 0.0000],\n",
              "         [0.6135, 0.7740, 0.0000,  ..., 0.0000, 1.0772, 0.0426],\n",
              "         ...,\n",
              "         [3.1985, 2.0426, 0.0976,  ..., 0.0426, 1.6229, 0.0000],\n",
              "         [3.3219, 1.8836, 1.3785,  ..., 0.0000, 0.4114, 0.0841],\n",
              "         [1.8992, 3.7876, 0.1110,  ..., 0.0000, 0.2750, 0.0426]]),\n",
              " tensor([[0.4436, 0.0000, 0.6041,  ..., 2.6645, 2.6182, 0.0000],\n",
              "         [0.0000, 0.0000, 1.3448,  ..., 2.5311, 2.4751, 0.0000],\n",
              "         [0.3561, 0.0000, 4.0652,  ..., 0.0000, 0.4854, 0.0000],\n",
              "         ...,\n",
              "         [0.0000, 0.0000, 0.5558,  ..., 0.6690, 1.8953, 0.0000],\n",
              "         [0.9635, 0.0000, 0.9855,  ..., 1.0356, 3.1473, 1.2750],\n",
              "         [0.2016, 0.4436, 0.9635,  ..., 1.8875, 3.1619, 0.3334]]))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "genes, labels = next(iter(gene_train_loader))\n",
        "genes, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[4.1993, 3.7506, 0.0000,  ..., 0.0000, 0.8639, 0.0000],\n",
              "         [2.1890, 0.6135, 0.2869,  ..., 0.0000, 1.6915, 0.1110],\n",
              "         [3.0054, 1.4489, 1.3561,  ..., 0.0566, 0.7312, 0.0000],\n",
              "         ...,\n",
              "         [2.8074, 3.0772, 1.0356,  ..., 0.0000, 0.9486, 0.0426],\n",
              "         [2.6206, 3.4463, 0.2510,  ..., 0.8156, 1.9635, 0.0000],\n",
              "         [2.5286, 1.5801, 0.9411,  ..., 0.0000, 0.2265, 0.0000]]),\n",
              " tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1763, 1.2388, 0.0000],\n",
              "         [0.5945, 0.0000, 1.2327,  ..., 4.9284, 1.9146, 0.0000],\n",
              "         [0.0000, 0.0000, 2.0072,  ..., 3.8115, 2.5311, 0.0000],\n",
              "         ...,\n",
              "         [0.7570, 0.6871, 0.6041,  ..., 3.2987, 2.3813, 0.8953],\n",
              "         [0.0000, 0.0000, 1.8914,  ..., 6.3420, 2.9598, 0.4957],\n",
              "         [0.2750, 1.2265, 0.5753,  ..., 1.2987, 3.3909, 0.0000]]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_genes, test_labels = next(iter(gene_test_loader))\n",
        "test_genes, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Todo: Add code to load VAE or latent features outputed from the VAE for gtex_gene_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DuRG3tVq268",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Building the model\n",
        "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5NZoEyq269",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "### Initialize the model, evaluator and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.2+cu102\n",
            "10.2\n",
            ">> Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\">> Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = gene_train_loader\n",
        "test_loader = gene_test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 24949415848 bytes. Error code 12 (Cannot allocate memory)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis DNN.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m model \u001b[39m=\u001b[39m Net()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# device = torch.device('cuda')  # use cuda or cpu\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# model.to(device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n",
            "\u001b[1;32m/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis DNN.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m activation_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m num_hidden \u001b[39m=\u001b[39m (NUM_FEATURES \u001b[39m+\u001b[39m NUM_OUTPUT) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(NUM_FEATURES, num_hidden),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     activation_fn(),\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     nn\u001b[39m.\u001b[39;49mLinear(num_hidden, NUM_OUTPUT),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     activation_fn()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1.hpc.dtu.dk/zhome/f1/e/155647/Prediction-of-protein-isoforms-using-semi-supervised-learning/DNN/ioannis%20DNN.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n",
            "File \u001b[0;32m~/Prediction-of-protein-isoforms-using-semi-supervised-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:83] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 24949415848 bytes. Error code 12 (Cannot allocate memory)"
          ]
        }
      ],
      "source": [
        "NUM_FEATURES = 2000\n",
        "NUM_OUTPUT = 156958\n",
        "\n",
        "# define network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        activation_fn = nn.ReLU\n",
        "        num_hidden = (NUM_FEATURES + NUM_OUTPUT) // 4\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(NUM_FEATURES, num_hidden),\n",
        "            activation_fn(),\n",
        "            nn.Linear(num_hidden, NUM_OUTPUT),\n",
        "            activation_fn()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = Net()\n",
        "# device = torch.device('cuda')  # use cuda or cpu\n",
        "# model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "# # Test the forward pass with dummy data\n",
        "# out = model(torch.randn(2, 3, 32, 32, device=device))\n",
        "# print(\"Output shape:\", out.size())\n",
        "# print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n",
        "# print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# num_epochs = 10\n",
        "# validation_every_steps = 500\n",
        "\n",
        "# step = 0\n",
        "# model.train()\n",
        "\n",
        "# train_accuracies = []\n",
        "# valid_accuracies = []\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "\n",
        "#     train_accuracies_batches = []\n",
        "\n",
        "#     for inputs, targets in train_loader:\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "#         # Forward pass, compute gradients, perform one training step.\n",
        "#         # Your code here!\n",
        "\n",
        "#         # Forward pass.\n",
        "#         output = model(inputs)\n",
        "\n",
        "#         # Compute loss.\n",
        "#         loss = loss_fn(output, targets)\n",
        "\n",
        "#         # Clean up gradients from the model.\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Compute gradients based on the loss from the current batch (backpropagation).\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Take one optimizer step using the gradients computed in the previous step.\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Increment step counter\n",
        "#         step += 1\n",
        "\n",
        "#         # Compute accuracy.\n",
        "#         predictions = output.max(1)[1]\n",
        "#         train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "\n",
        "#         if step % validation_every_steps == 0:\n",
        "\n",
        "#             # Append average training accuracy to list.\n",
        "#             train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "\n",
        "#             train_accuracies_batches = []\n",
        "\n",
        "#             # Compute accuracies on validation set.\n",
        "#             valid_accuracies_batches = []\n",
        "#             total_loss = 0\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 model.eval()\n",
        "#                 for inputs, targets in test_loader:\n",
        "#                     inputs, targets = inputs.to(device), targets.to(device)\n",
        "#                     output = model(inputs)\n",
        "#                     loss = loss_fn(output, targets)\n",
        "#                     total_loss += loss\n",
        "\n",
        "#                     predictions = output.max(1)[1]\n",
        "\n",
        "#                     # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "#                     valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "#                 model.train()\n",
        "\n",
        "#             # Append average validation accuracy to list.\n",
        "#             valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
        "\n",
        "#             print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "#             print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "#             print(f\"             total loss: {total_loss}\")\n",
        "\n",
        "# print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
